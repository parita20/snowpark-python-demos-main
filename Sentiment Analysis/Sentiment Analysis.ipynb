{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import datetime as _hex_datetime\n",
    "import json as _hex_json\n",
    "\n",
    "import pandas as _hex_pandas"
   ],
   "execution_count": null,
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "hex_scheduled = _hex_json.loads(\"false\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "hex_timezone = _hex_json.loads(\"\\\"America/Toronto\\\"\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "hex_project_id = _hex_json.loads(\"\\\"4a2d9aab-1e21-476c-9dec-0b5f7c103158\\\"\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "from datetime import date\n",
    "from tokenize import String\n",
    "\n",
    "import cachetools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import snowflake.snowpark\n",
    "import spacy\n",
    "from joblib import dump\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.functions import table_function, udf, udtf\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import (DateType, IntegerType, PandasSeries,\n",
    "                                      PandasSeriesType, StringType,\n",
    "                                      StructField, StructType, VariantType)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import hextoolkit\n",
    "\n",
    "hex_snowflake_conn = hextoolkit.get_data_connection('sfpscogs-simon')\n",
    "session = hex_snowflake_conn.get_snowpark_session()\n",
    "\n",
    "session.use_schema('public')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create file format to ingest training data\n",
    "session.sql('''\n",
    "    create or replace file format ff_pipe\n",
    "        type = CSV\n",
    "        field_delimiter = '|'\n",
    "''')\n",
    "\n",
    "# create the stage for python and model data\n",
    "session.sql('create or replace stage raw_data').collect()\n",
    "session.sql('create or replace stage model_data').collect()\n",
    "session.sql('create or replace stage python_load').collect()\n",
    "\n",
    "# create the directory stage for the data\n",
    "session.sql('create or replace stage raw_data_stage directory = (enable = true)').collect()\n",
    "\n",
    "# upload the unstructured file and stop words to the stages\n",
    "session.file.put('en_core_web_sm.zip','@model_data',overwrite=True)\n",
    "session.file.put('training_data.txt','@raw_data',auto_compress=False,overwrite=True)\n",
    "\n",
    "session.file.put('new_reviews.txt','@raw_data',auto_compress=False,overwrite=True)\n",
    "\n",
    "# refresh the stage\n",
    "session.sql('alter stage raw_data_stage refresh').collect()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "session.sql('''\n",
    "    create or replace table training_data as\n",
    "        select  \n",
    "            $2 product_id, \n",
    "            to_date($3) reviewdate,\n",
    "            $4 reviewtext, \n",
    "            $5 sentiment\n",
    "        from \n",
    "            @raw_data/training_data.txt (file_format => 'ff_pipe')\n",
    "        where product_id is not null;\n",
    "''').collect()\n",
    "\n",
    "session.sql('''\n",
    "    create or replace table new_reviews as\n",
    "        select  \n",
    "            $1 prodict_id,\n",
    "            to_date($2) reviewdate, \n",
    "            $3 reviewtext\n",
    "    from \n",
    "        @raw_data/new_reviews.txt (file_format => 'ff_pipe');\n",
    "''').collect()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(status='Table NEW_REVIEWS successfully created.')]"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Data Prep\n\nWe'll start this demo by first building our sentiment model, in order to do this we have a set of training data containing previous reviews and their classification for sentiment that requires cleaning and transforming.First we'll need to refine the text (remove punctuation, stopwords etc.) and then we'll want to make the sentiment classification more suitable for our algorithm.\n\n---\n\nTo get started, lets take a look at the training data we have:\n\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "session.table(\"TRAINING_DATA\").show(20)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "And check the distribution of data:\n\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = session.table('TRAINING_DATA') \\\n",
    "    .group_by(F.col('SENTIMENT')) \\\n",
    "    .agg(F.count(F.col('PRODUCT_ID')).alias('COUNT')).to_pandas()\n",
    "\n",
    "sns.set(rc={'figure.figsize':(20,8)})\n",
    "sns.barplot(x='SENTIMENT',y='COUNT',data=df)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We can see we have various reviews for products with their corresponding sentiment classification.\n\n---\n\nThe first transformation will be to process the review text. To do this we create a UDF that will perform the following:\n\n- Remove stop words\n- Remove punctuation\n- Remove currency values\n- Lemmatize the text\n\n\nNote that we create a vectorized UDF, so we can take advantage of batch processing in the UDF, additionally we cache the stopwords lexicon for better performance.\n\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "import spacy\n",
    "\n",
    "session.add_import('@model_data/en_core_web_sm.zip')\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def load_file(import_dir):\n",
    "    input_file = import_dir + 'en_core_web_sm.zip'\n",
    "    output_dir = '/tmp/en_core_web_sm' + str(os.getpid())\n",
    "            \n",
    "    with zipfile.ZipFile(input_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "        \n",
    "    return spacy.load(output_dir + \"/en_core_web_sm/en_core_web_sm-2.3.0\")    \n",
    "\n",
    "@udf(name='remove_stopwords_vect',packages=['spacy==2.3.5','cachetools'], session=session, is_permanent=True, replace=True, max_batch_size=10000,stage_location='python_load',)\n",
    "def remove_stopwords_vect(raw_text: PandasSeries[str]) -> PandasSeries[str]:\n",
    "    nlp = load_file(sys._xoptions['snowflake_import_directory'])\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for s in raw_text:\n",
    "        doc = nlp(s)\n",
    "        text = [str(t.lemma_) for t in doc if  \n",
    "                t not in stop_words \n",
    "                and not t.is_punct \n",
    "                and not t.is_currency\n",
    "                and t.lemma_ != '-PRON-']\n",
    "        text = list(map(lambda x: x.replace(' ', ''), text))\n",
    "        text = list(map(lambda x: x.replace('\\n', ''), text))\n",
    "        result.append(' '.join(token.lower() for token in text))\n",
    "    \n",
    "    return pd.Series(result)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "session.sql('''\n",
    "    select \n",
    "        remove_stopwords_vect(\n",
    "                'This surfboard is amazing! It\\\\'s a perfect length and weight, and I really like the design. \n",
    "                 If I was looking for a board to start learning on, this would make a great option. \n",
    "                 Really good value for money for $900') as processed_text\n",
    "''').collect()[0]['PROCESSED_TEXT']\n",
    "#str_sentiment = df.iat[0,0]\n",
    "#print(str_sentiment)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "The next transformation we'll need to do is convert the string value for sentiment into a numeric value, in order to make it more optimized for our ML algorithm.To do this we can create a simple UDF to bin the sentiment string to a value.\n\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Create and upload the UDF to bin the rating to sentiment \n",
    "@udf(name='convert_rating',\n",
    "     is_permanent=True,\n",
    "     replace=True,\n",
    "     stage_location='python_load')\n",
    "\n",
    "def convert_rating(x: str) -> int:\n",
    "    if x == 'NEGATIVE':\n",
    "        return -1\n",
    "    elif x == 'NEUTRAL':\n",
    "        return 0\n",
    "    elif x == 'POSITIVE':\n",
    "        return 1"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "With these UDFs we can now run a query and see what our data will look like for training:\n\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "df = session.table('TRAINING_DATA') \\\n",
    "    .filter(\n",
    "        F.col('REVIEWTEXT') != ''\n",
    "    ) \\\n",
    "    .select( \\\n",
    "        F.col('PRODUCT_ID'),\n",
    "        F.col('REVIEWDATE'),\n",
    "        F.call_udf(\n",
    "            'REMOVE_STOPWORDS_VECT',\n",
    "            F.col('REVIEWTEXT')).alias('PROCESSED_REVIEW'),\n",
    "        F.call_udf(\n",
    "            'CONVERT_RATING',\n",
    "            F.col('SENTIMENT')).alias('SENTIMENT')).show(20)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Model Training\n\nNext we want to train a model. Doing this in Snowflake is as simple are creating a Python Stored Procedure, which also allows us to re-run this when we want to retrain the model. Model training uses Snowflake Compute.The model will be saved to an internal stage, and can be used in a UDF for model inference within Snowflake.\n\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Create and upload a stored proc to train our sentiment model  \n",
    "def train_sentiment_model(session: snowflake.snowpark.Session) -> float:        \n",
    "    # build a pd with review data\n",
    "    df = session.table('TRAINING_DATA') \\\n",
    "        .filter(\n",
    "            F.col('REVIEWTEXT') != '') \\\n",
    "        .select(\n",
    "            F.call_udf(\n",
    "                'REMOVE_STOPWORDS_VECT',\n",
    "                F.col('REVIEWTEXT')).alias('PROCESSED_TEXT'),\n",
    "            F.call_udf(\n",
    "                'CONVERT_RATING',\n",
    "                F.col('SENTIMENT')).alias('SENTIMENT')).toPandas()\n",
    "    \n",
    "    index = df.index\n",
    "    df['RANDOM'] = np.random.randn(len(index))\n",
    "    train = df[df['RANDOM'] <= 0.8] # 0.8\n",
    "    test = df[df['RANDOM'] > 0.8] # 0.8\n",
    "    \n",
    "    # vectorize the data\n",
    "    vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "    train_matrix = vectorizer.fit_transform(train['PROCESSED_TEXT'])\n",
    "    test_matrix = vectorizer.transform(test['PROCESSED_TEXT'])\n",
    "    \n",
    "    # split feature and label \n",
    "    x_train = train_matrix\n",
    "    x_test = test_matrix\n",
    "    y_train = train['SENTIMENT']\n",
    "    y_test = test['SENTIMENT']\n",
    "    \n",
    "    # Logistic Regression Model\n",
    "    lr = LogisticRegression(multi_class='multinomial', max_iter=10000)\n",
    "    lr.fit(x_train,y_train)\n",
    "    predictions = lr.predict(x_test)\n",
    "\n",
    "    model_output_dir = '/tmp'\n",
    "\n",
    "    # Save model file\n",
    "    model_file = os.path.join(model_output_dir, 'model.joblib')\n",
    "    dump(lr, model_file)\n",
    "    session.file.put(model_file, \"@model_data\",overwrite=True)\n",
    "\n",
    "    # Save vectorizer file\n",
    "    vect_file = os.path.join(model_output_dir, 'vectorizer.joblib')\n",
    "    dump(vectorizer, vect_file)\n",
    "    session.file.put(vect_file, \"@model_data\",overwrite=True)\n",
    "\n",
    "    return accuracy_score(y_test, predictions)\n",
    "\n",
    "# Register the Stored Procedure\n",
    "session.sproc.register(name='train_sentiment_model',\n",
    "                       func=train_sentiment_model, \n",
    "                       packages=['snowflake-snowpark-python','pandas','scikit-learn', 'joblib'],\n",
    "                       replace=True, \n",
    "                       is_permanent=True,\n",
    "                       stage_location='python_load')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "session.sql('use warehouse data_science').collect()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = session.table('TRAINING_DATA') \\\n",
    "    .filter(\n",
    "        F.col('REVIEWTEXT') != '') \\\n",
    "    .select(\n",
    "        F.call_udf(\n",
    "            'REMOVE_STOPWORDS_VECT',\n",
    "            F.col('REVIEWTEXT')).alias('PROCESSED_TEXT'),\n",
    "        F.call_udf(\n",
    "            'CONVERT_RATING',\n",
    "            F.col('SENTIMENT')).alias('SENTIMENT')).toPandas()\n",
    "\n",
    "len(df)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "session.call('TRAIN_SENTIMENT_MODEL')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Model Deployment\n\nWith the model artifact produced from the Stored Procedure, we can create a UDF that can be used to infer sentiment for future data ingested into Snowflake.\n\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "session.clear_packages()\n",
    "session.clear_imports()\n",
    "session.add_import('@MODEL_DATA/model.joblib.gz')\n",
    "session.add_import('@MODEL_DATA/vectorizer.joblib.gz')\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def load_model(file_name):\n",
    "    model_file_path = sys._xoptions.get(\"snowflake_import_directory\") + file_name\n",
    "    return load(model_file_path)\n",
    "\n",
    "columns = ('NEGATIVE','NEUTRAL','POSITIVE')\n",
    "    \n",
    "@udf(name='predict_sentiment_vect',\n",
    "     is_permanent=True,\n",
    "     replace=True,\n",
    "     stage_location='python_load',\n",
    "     max_batch_size=1000,\n",
    "     input_types=[PandasSeriesType(StringType())], \n",
    "     return_type=PandasSeriesType(VariantType()),\n",
    "     packages=['pandas','scikit-learn','cachetools','joblib'])     \n",
    "def predict_sentiment_vector(sentiment_str):  \n",
    "    model = load_model('model.joblib.gz')\n",
    "    vectorizer = load_model('vectorizer.joblib.gz')                            \n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for s in sentiment_str:        \n",
    "        matrix = vectorizer.transform([s])\n",
    "        \n",
    "        df = pd.DataFrame(model.predict_proba(matrix),columns=columns)\n",
    "                \n",
    "        response = df.loc[0].to_json()\n",
    "        result.append(json.loads(response))\n",
    "        \n",
    "    return pd.Series(result)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We can quickly test our UDF with a simple SQL call:\n\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "session.sql('''\n",
    "    select predict_sentiment_vect('PRACTICALLY PERFECT IN EVERY WAY') sentiment\n",
    "''').show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Scoring new data\n\nEarlier on, we ingested a seperate dataset of reviews that had no sentiment associated. We’re going to use the model and UDF to now score these new records.\n\nFirst we’re going to take our new data table and run it through our text processing UDF:\n\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "session.table('new_reviews').select(\n",
    "    F.col('product_id'),\n",
    "    F.col('review_date'),\n",
    "    F.col('product_review'), \n",
    "    F.call_udf(\n",
    "        'remove_stopwords_vect',\n",
    "        F.col('product_review')).alias('processed_review')    \n",
    ").write.save_as_table('new_reviews_processed',mode=\"overwrite\", table_type=\"temporary\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = session.table('new_reviews_processed').select(\n",
    "    F.col('product_id'),\n",
    "    F.col('review_date'),\n",
    "    F.col('product_review'),\n",
    "    F.col('processed_review'),\n",
    "    F.call_udf(\n",
    "        'predict_sentiment_vect',\n",
    "        F.col('processed_review')).alias('sentiment'))\n",
    "\n",
    "df = df.select(\n",
    "    F.col('product_id'),\n",
    "    F.col('review_date'),\n",
    "    F.col('product_review'),\n",
    "    F.col('processed_review'),\n",
    "    F.col('sentiment')['NEGATIVE'].alias('negative'),\n",
    "    F.col('sentiment')['NEUTRAL'].alias('neutral'),    \n",
    "    F.col('sentiment')['POSITIVE'].alias('positive')\n",
    ").write.save_as_table('new_reviews_scored',mode=\"overwrite\")\n",
    "\n",
    "session.table('new_reviews_scored').select(\n",
    "    F.col('product_id'),\n",
    "    F.col('review_date'),\n",
    "    F.col('product_review'),  \n",
    "    F.col('positive'),\n",
    "    F.col('neutral'),\n",
    "    F.col('negative')).show(50)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "hex_info": {
   "author": "Simon Coombes",
   "project_id": "4a2d9aab-1e21-476c-9dec-0b5f7c103158",
   "version": "draft",
   "exported_date": "Thu Jan 26 2023 14:49:04 GMT+0000 (Coordinated Universal Time)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
